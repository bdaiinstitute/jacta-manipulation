:py:mod:`learning.learner`
==========================

.. py:module:: learning.learner

.. autoapi-nested-parse::

   
   ``Learner`` module encapsulating the Deep Deterministic Policy Gradient (DDPG) algorithm.

   :class:`.Learner` initializes the actor, critic, and normalizers and takes care of checkpoints
   during training as well as network loading if starting from pre-trained networks.


Module Contents
---------------

.. py:class:: Learner(plant: jacta.planner.dynamics.simulator_plant.SimulatorPlant, graph: jacta.planner.core.graph.Graph, replay_buffer: jacta.learning.replay_buffer.ReplayBuffer, params: jacta.planner.core.parameter_container.ParameterContainer, save_local: bool = True, load_local: bool = False, verbose: bool = True)


   
   Deep Deterministic Policy Gradient algorithm class.

   Uses a state/goal normalizer and the HER sampling method to solve sparse reward environments.
   .. py:method:: reset() -> None


   .. py:method:: actor_actions(actor: jacta.learning.networks.Actor, node_ids: torch.IntTensor, action_time_step: float) -> torch.FloatTensor


   .. py:method:: relative_distances_to(data_container: Union[jacta.planner.core.graph.Graph, jacta.learning.replay_buffer.ReplayBuffer], ids: torch.IntTensor, target_states: torch.FloatTensor) -> torch.FloatTensor


   .. py:method:: reward_function(data_container: Union[jacta.planner.core.graph.Graph, jacta.learning.replay_buffer.ReplayBuffer], node_ids: torch.FloatTensor, goals: torch.FloatTensor) -> Tuple[torch.FloatTensor, torch.FloatTensor]


   .. py:method:: planner_exploration(root_states: torch.FloatTensor) -> torch.FloatTensor


   .. py:method:: update_norm(states: torch.FloatTensor, goals: torch.FloatTensor) -> None

      
      Update the normalizers with the current episode of play experience.

      Samples the trajectory instead of taking every experience to create a goal distribution that
      is equal to what the networks encouter.

   .. py:method:: policy_rollout(temporary: bool = False) -> Tuple[torch.FloatTensor, bool]


   .. py:method:: graph_rollout(temporary: bool = False) -> torch.FloatTensor


   .. py:method:: set_demonstration_injection(final_success_rate: float) -> None


   .. py:method:: train(num_epochs: int = 50) -> None

      
      Train a policy to solve the environment with DDPG.

      Trajectories are resampled with HER to solve sparse reward environments.

      `DDPG paper <https://arxiv.org/pdf/1509.02971.pdf>`_

      `HER paper <https://arxiv.org/pdf/1707.01495.pdf>`_

   .. py:method:: state_action_training_data(num_trajectories: int = 1000, discount_factor: float = 0.98) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]


   .. py:method:: pretrain(num_epochs: int = 100, num_trajectories: int = 1000, train_critic: bool = True) -> None


   .. py:method:: eval_agent() -> Tuple[float, float, float]

      
      Evaluate the current agent performance on the task.

      Runs ``learner_evals`` times and averages the success rate.

   .. py:method:: save_models() -> None

      
      Save the actor and critic networks and the normalizers.

      Saves are located under ``/models/<model_filename>/``.

   .. py:method:: load_models(path: str) -> None

      
      Load the actor and critic networks and the normalizers.


