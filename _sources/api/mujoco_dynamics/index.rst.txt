:py:mod:`mujoco_dynamics`
=========================

.. py:module:: mujoco_dynamics


Module Contents
---------------

.. py:function:: get_joint_dimensions(joint_ids: numpy.typing.ArrayLike, state_address: numpy.typing.ArrayLike, state_length: int) -> torch.IntTensor

   
   Given a list of joint ids, and the list of addresses in the states for the joints.
   We return the dimensions in the state corresponding to the list of joint ids.

.. py:function:: decompose_state_dimensions(model: mujoco.MjModel) -> Tuple[torch.IntTensor, torch.IntTensor, torch.IntTensor, torch.IntTensor]

   
   Decompose the states indices in 4 groups, split between positions or velocities and
   actuated or not actuated.

.. py:class:: MujocoPlant(params: jacta.planner.core.parameter_container.ParameterContainer)


   Bases: :py:obj:`jacta.planner.dynamics.simulator_plant.SimulatorPlant`

   .. py:method:: reset() -> None


   .. py:method:: initialize(params: jacta.planner.core.parameter_container.ParameterContainer) -> None


   .. py:method:: dynamics(states: torch.FloatTensor, actions: torch.FloatTensor, action_time_step: float) -> Tuple[torch.FloatTensor, torch.FloatTensor]

      
      Conditions on the size of the states/actions and calls the appropriate singular or parallel dynamics.

      :param states: (nx,) or (num_envs, nx) sized vector of states
      :param actions: (2, na) or (num_envs, 2, na) array containing the start and end action vectors
                      of the desired trajectory.
      :param action_time_step: the hold time for the action.

      :returns: A tuple of (next state, intermediate states)

   .. py:method:: get_num_substeps(time_step: Optional[float] = None) -> int


   .. py:method:: get_gradient_placeholders(size: Optional[int] = None) -> Tuple[numpy.typing.ArrayLike, numpy.typing.ArrayLike, numpy.typing.ArrayLike, numpy.typing.ArrayLike]


   .. py:method:: get_sub_stepped_gradients(state: torch.FloatTensor, action: torch.FloatTensor, num_substeps: Optional[int] = None) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]


   .. py:method:: get_gradients(states: torch.FloatTensor, actions: torch.FloatTensor) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      
      Computes the dynamics gradients.

      :param states: (nx,) or (num_envs, nx) sized vector of states
      :param actions: (na,) or (num_envs, na) array containing the action vectors.

      :returns: *state_gradients_state* -- (nx, nx) or (num_envs, nx, nx),
                state_gradients_control: (nx, nu) or (num_envs, nx, nu),
                sensor_gradients_state: (ns, nx) or (num_envs, ns, nx),
                sensor_gradients_control: (ns, nu) or (num_envs, ns, nu),

   .. py:method:: set_state(state: torch.FloatTensor) -> None


   .. py:method:: get_state() -> torch.FloatTensor


   .. py:method:: set_action(action: torch.FloatTensor) -> None


   .. py:method:: get_action() -> torch.FloatTensor


   .. py:method:: update_sensor() -> None


   .. py:method:: get_sensor(states: torch.FloatTensor) -> torch.FloatTensor

      
      Update the sensor measurement held by plant.data.
      This only supports position- and velocity-based sensors, NOT ACCLERATION-BASED sensors.
      We use the minimal set of computations extracted from mj_step1, see the link below for more details:
      https://mujoco.readthedocs.io/en/latest/programming/simulation.html?highlight=mj_step1#simulation-loop
      Finally, returns the sensor measurement.

      :param states: (nx,) or (num_envs, nx) sized vector of states

      :returns: sensor (nsensordata,) or (num_envs, nsensordata)

   .. py:method:: state_difference(s1: torch.FloatTensor, s2: torch.FloatTensor, h: float = 1.0) -> torch.FloatTensor

      
      Compute finite-difference velocity given two state vectors and a time step h
      ds = (s2 - s1) / h

   .. py:method:: state_addition(s1: torch.FloatTensor, ds: torch.FloatTensor, h: float = 1.0) -> torch.FloatTensor

      
      Integrate forward a state s with a velocity ds for a time step h.
      s2 = s1 + h * ds

   .. py:method:: get_mass() -> float


   .. py:method:: get_quat_indices() -> None

      
      Stores the indices of the state corresponding to quaternion in class field.

   .. py:method:: normalize_state(states: torch.FloatTensor) -> torch.FloatTensor

      
      :param states: (num_envs, nx) tensor of randomly sampled states
      :type states: torch.FloatTensor

      Outputs:
          torch.FloatTensor: (num_envs, nx) tensor with quaternion portions normalized

   .. py:method:: get_collision_free(states: torch.FloatTensor) -> Optional[torch.FloatTensor]

      
      :param state: (num_envs, nx) tensor of randomly sampled states
      :type state: FloatTensor

      Outputs:
          Optional[FloatTensor]: tensor of all collision free states (or None if none exist)

   .. py:method:: visualize_state(state: torch.FloatTensor, display_time: float = 5.0) -> None


   .. py:method:: visualize_trajectory(trajectory: torch.FloatTensor, display_time: float = 5.0, time_step: Optional[float] = None) -> None



