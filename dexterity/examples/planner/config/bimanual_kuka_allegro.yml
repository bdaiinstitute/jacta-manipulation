# Copyright (c) 2023 Boston Dynamics AI Institute LLC. All rights reserved.

model_filename: dexterity/models/xml/scenes/legacy/bimanual_kuka_allegro.xml

action:
  bound_lower: ([
        -2.81706,
        -1.9444,
        -2.81706,
        -1.9444,
        -2.81706,
        -1.9444,
        -2.90433,
        -2.81706,
        -1.9444,
        -2.81706,
        -1.9444,
        -2.81706,
        -1.9444,
        -2.90433,
        -0.47,
        -0.196,
        -0.174,
        -0.227,
        -0.47,
        -0.196,
        -0.174,
        -0.227,
        -0.47,
        -0.196,
        -0.174,
        -0.227,
        +0.263,
        -0.105,
        -0.189,
        -0.162,
        -0.47,
        -0.196,
        -0.174,
        -0.227,
        -0.47,
        -0.196,
        -0.174,
        -0.227,
        -0.47,
        -0.196,
        -0.174,
        -0.227,
        +0.263,
        -0.105,
        -0.189,
        -0.162])
  bound_upper: ([
        2.81706,
        1.9444,
        2.81706,
        1.9444,
        2.81706,
        1.9444,
        2.90433,
        2.81706,
        1.9444,
        2.81706,
        1.9444,
        2.81706,
        1.9444,
        2.90433,
        +0.47,
        +1.610,
        +1.709,
        +1.618,
        +0.47,
        +1.610,
        +1.709,
        +1.618,
        +0.47,
        +1.610,
        +1.709,
        +1.618,
        +1.396,
        +1.163,
        +1.644,
        +1.719,
        +0.47,
        +1.610,
        +1.709,
        +1.618,
        +0.47,
        +1.610,
        +1.709,
        +1.618,
        +0.47,
        +1.610,
        +1.709,
        +1.618,
        +1.396,
        +1.163,
        +1.644,
        +1.719])
  range: torch.cat((torch.ones(14), (self.action_bound_upper[14:] - self.action_bound_lower[14:]) * 0.25))

reward:
  distance_scaling: torch.cat((torch.tensor([10, 10, 10, 2, 2, 2]), torch.zeros(98)))
  proximity_scaling: 0.1

start:
  state: ([0.6, 0, 0.275, 1, 0, 0, 0, 0, 0.2, 0, -1.0, 0, 0.6, 0, 0, 0.2, 0, -1.0, 0, 0.6, 0] + [0.0] * 84)
goal:
  state: ([0.6, 0.45, 0.275, 1, 0, 0, 0] + [0.0] * 98)

planner:
  single_goal:
    steps_per_goal: 1000
    max_main_nodes: 1000
