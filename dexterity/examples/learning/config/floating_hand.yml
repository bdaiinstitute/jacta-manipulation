# Copyright (c) 2023 Boston Dynamics AI Institute LLC. All rights reserved.

model_filename: dexterity/models/xml/scenes/legacy/floating_hand.xml

vis:
  filename: dexterity/models/xml/scenes/legacy/floating_hand.yml
  indices: ([0, 2, 1])
  scale: ([1, -1 / np.pi, 1])

action:
  bound_lower: ([-2, 0.2, -np.pi / 2, -np.pi / 2, 0.0, 0.0])
  bound_upper: ([2, 2, 0.0, 0.0, np.pi / 2, np.pi / 2])
  range: ([2, 2, 2, 4, 2, 4])

reward:
  distance_scaling: torch.cat((torch.tensor([1, 2, 1 / 2 * np.pi]), torch.zeros(6), torch.ones(3)*0.1, torch.ones(6)*0.0))

start:
  state:       ([   0, 0.15, 0,    0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
  bound_lower: ([-0.5, 0.15, 0, -0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
  bound_upper: ([ 0.5, 0.15, 0,  0.5, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

goal:
  state:       ([   0,  0.5, np.pi, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
  bound_lower: ([-0.5, 0.15, np.pi, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
  bound_upper: ([ 0.5,  0.5, np.pi, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

learner:
  early_stop: 1.0
  trajectory_length: 100
  use_sparse_reward: True
  final_success_distance: 0.05
  stop_converged_injection: False
  max_planner_experience_share: 0.5

intermediate_pruning: False
num_sub_goals: 30
steps_per_goal: 30
goal_bias: 0.0
termination_distance: 0.0
max_main_nodes: 50000
action_time_step: 0.4
action_steps_max: 3

seed: 0
