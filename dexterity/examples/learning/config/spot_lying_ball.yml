# Copyright (c) 2023 Boston Dynamics AI Institute LLC. All rights reserved.

model_filename: dexterity/models/xml/scenes/legacy/spot_lying_ball.xml

action:
  bound_lower: ([-0.5, -0.5, -2.5] * 4)
  bound_upper: ([0.5, 2.0, 0.5] * 4)
  range: ([0.4, 0.4, 0.4] * 4)
  types: "[AT.RANGED, AT.PROXIMITY, AT.CONTINUATION, AT.GOAL]"
  distribution: ([2, 3, 1, 3])

reward:
  distance_scaling: ([1, 1, 1, 2, 2, 0] + [0] * 12 + [0.1,0.1,0.1,0.1,0.1,0.1] + [0] *12)
  proximity_scaling: 0.1
  reachability_scaling: 0.1

# object xyz (3) + object quat wxyz (4) + robot joint pos (12) + object vel (6) + robot joint vel (12)
start:
  state:       ([0, 0, 0.7] + [np.cos(np.pi/4), 0,  np.sin(np.pi/4), 0] + [0.45, 1.0, -1.5, -0.45, 1.0, -1.5, 0.3, 0.6, -1.35, -0.3, 0.6, -1.35] + [0.0] * 18)
  bound_lower: ([0, 0, 0.7] + [              0, 0, -1, 0] + [0.45, 1.0, -1.5, -0.45, 1.0, -1.5, 0.3, 0.6, -1.35, -0.3, 0.6, -1.35] + [0.0] * 18)
  bound_upper: ([0, 0, 0.7] + [np.cos(np.pi/4), 0,  1, 0] + [0.45, 1.0, -1.5, -0.45, 1.0, -1.5, 0.3, 0.6, -1.35, -0.3, 0.6, -1.35] + [0.0] * 18)
goal:
  state:       ([   0,    0, 0.7] + [1, 0, 0, 0] + [0.45, 1.0, -1.5, -0.45, 1.0, -1.5, 0.3, 0.6, -1.35, -0.3, 0.6, -1.35] + [0.0] * 18)
  bound_lower: ([ 0.1, -0.1, 0.7] + [1, 0, 0, 0] + [0.45, 1.0, -1.5, -0.45, 1.0, -1.5, 0.3, 0.6, -1.35, -0.3, 0.6, -1.35] + [0.0] * 18)
  bound_upper: ([-0.1,  0.1, 0.9] + [1, 0, 0, 0] + [0.45, 1.0, -1.5, -0.45, 1.0, -1.5, 0.3, 0.6, -1.35, -0.3, 0.6, -1.35] + [0.0] * 18)

learner:
  early_stop: 1.0
  trajectory_length: 75
  use_sparse_reward: True
  final_success_distance: 0.2
  stop_converged_injection: False
  max_planner_experience_share: 0.5

ignore_sampled_state_collisions: True
intermediate_pruning: False
num_sub_goals: 30
steps_per_goal: 30
goal_bias: 0.0
termination_distance: 0.0
max_main_nodes: 50000
action_time_step: 0.4
action_steps_max: 3

seed: 0
