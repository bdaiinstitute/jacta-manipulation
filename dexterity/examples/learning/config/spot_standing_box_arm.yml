# Copyright (c) 2023 Boston Dynamics AI Institute LLC. All rights reserved.

model_filename: dexterity/models/xml/scenes/legacy/spot_standing_box.xml

vis:
  scale: ([2/np.pi, 2/np.pi, -2/np.pi])
  indices: ([9, 7, 8])

action:
  bound_lower: ([-1.5, -2.0, 0.1, -0.1, -1.5, -0.1, -1.545])
  bound_upper: ([ 1.5, 0.05, 3.0,  0.1,  1.5,  0.1,  -0.05])
  range:       ([ 0.3,  0.4, 0.5,  0.5,  0.5,  0.5,    0.5])
  types: ([AT.RANGED, AT.PROXIMITY, AT.CONTINUATION, AT.GOAL])
  distribution: ([0, 1, 1, 1])

reward:
  distance_scaling: torch.cat((torch.zeros(6), torch.ones(7), torch.zeros(6), torch.zeros(7)))
  proximity_scaling: 0.0
  reachability_scaling: 0.0

start:
  state:       ([2, 0, 0.275, 1, 0, 0, 0,  0.0, -0.9, 1.9,  0.0, -0.9,  0.0,  -1.54] + [0.0] * 13)
  bound_lower: ([2, 0, 0.275, 1, 0, 0, 0, -1.0, -1.5, 0.5, -0.1, -1.0, -0.1, -1.545] + [0.0] * 13)
  bound_upper: ([2, 0, 0.275, 1, 0, 0, 0,  1.0, -0.5, 2.5,  0.1,  0.5,  0.1,  -0.05] + [0.0] * 13)
goal:
  state:       ([2, 0, 0.275, 1, 0, 0, 0,  0.0, -0.9, 1.8,  0.0, -0.9,  0.0,  -1.54] + [0.0] * 13)
  bound_lower: ([2, 0, 0.275, 1, 0, 0, 0,  0.0, -0.9, 1.8,  0.0, -0.9,  0.0,  -1.54] + [0.0] * 13)
  bound_upper: ([2, 0, 0.275, 1, 0, 0, 0,  0.0, -0.9, 1.8,  0.0, -0.9,  0.0,  -1.54] + [0.0] * 13)

learner:
  early_stop: 0.9
  trajectory_length: 50
  use_sparse_reward: True
  final_success_distance: 0.05
  stop_converged_injection: False
  max_planner_experience_share: 0.5

intermediate_pruning: False
num_sub_goals: 10
steps_per_goal: 30
goal_bias: 0.0
termination_distance: 0.0
max_main_nodes: 50000
action_time_step: 0.4
action_steps_max: 3

seed: 0
